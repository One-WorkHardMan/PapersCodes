{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from six.moves import xrange\n",
    "import umap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/170498071 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c500d02e88924dad80e4d81dd4860228"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#加载Cifir-10\n",
    "training_set = datasets.CIFAR10(root=\"data\",train=True,download=True,transform=transforms.Compose([\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(1,1,1))\n",
    "]))\n",
    "\n",
    "validation_set = datasets.CIFAR10(root=\"data\",train=False,download=True,transform=transforms.Compose([\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5,0.5,0.5),(1,1,1))\n",
    "]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#VQ 层\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self,num_channels:int,embedding_dim:int,commitment_cost:float):\n",
    "        super.__init__()\n",
    "        self._embedding_dim  = embedding_dim\n",
    "        self._num_channels = num_channels\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_channels,self._embedding_dim)\n",
    "        # 给这个可学习的权重初始化，直接用这个权重作为Codebook了。这个weight: [numc,embeddim] = [k,D]\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_channels,1/self._num_channels)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,inputs:torch.Tensor):\n",
    "        inputs = inputs.permute(0,2,3,1).contiguous() #内存数据连续化\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        # 拉成二维\n",
    "        flat_input = inputs.view(-1,self._embedding_dim)\n",
    "\n",
    "        #计算距离 这里使用的事欧式距离。也是是余弦相似度。其实之间是有关系的。\n",
    "        distance = (torch.sum(flat_input**2,dim=1,keepdim=True)+\n",
    "                    torch.sum(self._embedding.weight**2,dim=1)-\n",
    "                    2*torch.matmul(flat_input,self._embedding.weight.t()))\n",
    "\n",
    "        #Ecoding\n",
    "        encoding_indices = torch.argmin(distance,dim=1).unsqueeze(1) #增加维度，从一行n个，变成 [n,1]\n",
    "        encodings = torch.zeros(encoding_indices.shape[0],self._num_channels,device=inputs.device) # 这个就是One-Hot阶段，生成对应的 [B*H*W,K]的过程\n",
    "\n",
    "        #生成OneHot向量，encodings前面全是 0 ，这里就是把对应的位置变成1.\n",
    "        encodings.scatter_(1,encoding_indices,1)\n",
    "\n",
    "        #Quantize and Unflatten, encodings 和 Codebook矩阵相乘，乘就来的就是 [BWH,D]这个D就是码本里面的数据，最近邻的那个中心。（聚类中心？？？）\n",
    "        quantized:torch.Tensor = torch.matmul(encodings,self._embedding_dim.weight).view(input_shape)\n",
    "\n",
    "        #Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(),inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized,inputs.detach())\n",
    "        loss = q_latent_loss + e_latent_loss * self._commitment_cost\n",
    "\n",
    "        #trick 因为编码器Inputs 没有办法进行梯度的传播，也就是连续求导，所以这样写，类似于参数重整化，一个道理。\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "\n",
    "        # 困惑度 ： 用来验证VQ是否在work ,这里就是信息熵。\n",
    "        avg_probs = torch.mean(encodings,dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs+1e-10)))\n",
    "\n",
    "        return loss,quantized.permute(0,3,1,2).contiguous(),perplexity,encodings\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#残差部分\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self,in_channels,num_hiddens,num_residual_hidden):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=num_residual_hidden,kernel_size=3,stride=1,padding=1,bias=False),\n",
    "\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=num_residual_hidden,out_channels=num_hiddens,kernel_size=1,stride=1,bias=False)\n",
    "        )\n",
    "    def forward(self,inputs):\n",
    "        return inputs + self.block(inputs)\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self,in_channels,num_hiddens,num_residual_hidden,num_residual_layers):\n",
    "        super().__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        # 这个写法 还挺新颖：\n",
    "        self._layers = nn.ModuleList([\n",
    "            [Residual(in_channels,num_hiddens,num_residual_hidden) for _ in range(self._num_residual_layers)]\n",
    "        ])\n",
    "    def forward(self,inputs):\n",
    "        for i in range(self._num_residual_layers):\n",
    "            inputs = self._layers[i](inputs)\n",
    "        return F.relu(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,in_channels,num_hiddens,num_residual_hidden,num_residual_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self._conv1 = nn.Conv2d(in_channels,num_hiddens//2,kernel_size=4,stride=2,padding=1)\n",
    "        self._conv2 = nn.Conv2d(in_channels=num_hiddens//2,out_channels=num_hiddens,kernel_size=4,stride=2,padding=1)\n",
    "        self._conv3 = nn.Conv2d(in_channels=num_hiddens,out_channels=num_hiddens,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self._residual_stack = ResidualStack(in_channels=num_hiddens,num_hiddens=num_hiddens,num_residual_layers = num_residual_layers,num_residual_hidden=num_residual_hidden)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        x = self._conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self._conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self._conv3(x)\n",
    "\n",
    "        return self._residual_stack(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,in_channels,num_hiddens,num_residual_hidden,num_residual_layers):\n",
    "        self._conv1 = nn.Conv2d(in_channels=in_channels,out_channels=num_hiddens,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self._redidual_stack = ResidualStack(in_channels=num_hiddens,num_hiddens=num_hiddens,num_residual_layers=num_residual_layers,num_residual_hidden=num_residual_hidden)\n",
    "\n",
    "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,\n",
    "                                                out_channels=num_hiddens//2,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=2,\n",
    "                                                padding=1)\n",
    "\n",
    "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n",
    "                                                out_channels=3,\n",
    "                                                kernel_size=4,\n",
    "                                                stride=2,\n",
    "                                                padding=1)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        x = self._conv1(inputs)\n",
    "\n",
    "        x = self._redidual_stack(x)\n",
    "\n",
    "        x = self._conv_trans_1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        return self._conv_trans_2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Train\n",
    "batch_size = 256\n",
    "num_training_updates = 15000\n",
    "\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "\n",
    "embedding_dim = 64\n",
    "# codebook 的 向量的条数；\n",
    "num_embeddings = 512\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "learning_rate = 1e-3\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_loader = DataLoader(\n",
    "    training_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,num_hiddens,num_residual_layers,num_residual_hiddens,num_embeddings,embedding_dim,commitment_cost):\n",
    "        super(Model, self).__init__()\n",
    "        self._encoder = Encoder(3,num_hiddens=num_hiddens,num_residual_hidden=num_residual_hiddens,num_residual_layers=num_residual_layers)\n",
    "\n",
    "        # 进入VQ之前的后处理，感觉处不处理没有区别。。。\n",
    "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,out_channels=embedding_dim,kernel_size=1,stride=1)\n",
    "        # VQ传的参数：Codebook 的 条数，维度；\n",
    "        self._vq_vae = VectorQuantizer(num_channels=num_embeddings,embedding_dim=embedding_dim,commitment_cost=commitment_cost)\n",
    "\n",
    "        self._decoder = Decoder(in_channels=embedding_dim,num_hiddens=num_hiddens,num_residual_layers=num_residual_layers,num_residual_hidden=num_residual_hiddens)\n",
    "\n",
    "    def forward(self,inputs:torch.Tensor):\n",
    "        z = self._encoder(inputs)\n",
    "        z = self._pre_vq_conv(z)\n",
    "        loss,quantized,perplexity,_ = self._vq_vae(z)\n",
    "        x_recon = self._decoder(quantized)\n",
    "\n",
    "        return loss,x_recon,perplexity\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Model(num_hiddens,num_residual_layers,num_residual_hiddens,num_embeddings,embedding_dim,commitment_cost).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate,amsgrad=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in range(num_training_updates):\n",
    "    (data,_) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    vq_loss,data_recon,perplexity = model(data)\n",
    "    recon_error = F.mse_loss(data_recon,data)\n",
    "\n",
    "    loss = recon_error + vq_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item()) # item() 就是获取集合的所有元素，并且转换成元祖列表[( , )]\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if(i+1) % 100 ==0:\n",
    "        print('%d iteration' % (i+1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_res_recon_error_smooth = savgol_filter(train_res_recon_error,201,7)\n",
    "train_res_perplexity_smooth = savgol_filter(train_res_perplexity,201,7)\n",
    "\n",
    "\n",
    "f= plt.figure( figsize=( 16,8))\n",
    "ax =f.add_subplot ( 1,2,1)\n",
    "ax.plot ( train_res_recon_error_smooth)\n",
    "ax.set_yscale( ' log' )\n",
    "ax.set_title( 'smoothed NMSE. ')\n",
    "ax.set_xlabel ( 'iteration ')\n",
    "\n",
    "\n",
    "ax =f.add_subplot ( 1,2,2)\n",
    "ax.plot (train_res_perplexity_smooth)\n",
    "ax.set_title( 'Smoothed Average codebook usage (perplexity) . ' )\n",
    "ax.set_xlabei ('ieration')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#model eval\n",
    "model.eval()\n",
    "\n",
    "(valid_originals,_) = next (iter(validation_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
    "_, valid_quantize,_,_= model._vq_vae(vq_output_eval)\n",
    "valid_reconstructions = model._decoder (valid_quantize)\n",
    "\n",
    "\n",
    "( train_originals,_) = next ( iter(training_loader) )\n",
    "train_originals = train_originals.to(device)\n",
    "_, train_reconstructions,_,_ = model._vq_vae(train_originals)\n",
    "def show ( img ) :\n",
    "    npimg = img.numpy ()\n",
    "    fig = plt.imshow(np.transpose(npimg,(1,2,0 )),interpolation='nearest ' )\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "show ( make_grid(valid_reconstructions.cpu ( ).data)+0.5, )\n",
    "show ( make_grid(valid_originals.cpu( )+0.5))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#View Embedding\n",
    "proj = umap.UMAP (n_neighbors=3,\n",
    "                    min_dist=0.1,\n",
    "metric='cosine ' ).fit_transform(model._vg_vae._embedding.weight.data.cpu())\n",
    "\n",
    "plt.scatter(proj[ :,0], proj[:,1],alpha=0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}